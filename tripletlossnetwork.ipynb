{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.python.data.experimental import AUTOTUNE\n",
    "\n",
    "from tripletdataset import TripletDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TripletDataset(root_path='/mnt/d/cardimagescans/', set_codes=['khm', 'isd', 'sta', 'ogw'], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a triplet loss model the Keras way\n",
    "input_shape=(350,250,3)\n",
    "\n",
    "class DistanceLayer(ks.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        distance_ap = tf.reduce_sum(tf.square(anchor-positive), -1)\n",
    "        distance_an = tf.reduce_sum(tf.square(anchor-negative), -1)\n",
    "        return (distance_ap, distance_an)\n",
    "\n",
    "class SiameseModel(ks.Model):\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = ks.metrics.Mean(name='loss')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "        \n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {'loss':self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {'loss': self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "\n",
    "vgg = ks.applications.VGG19(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        pooling='max'\n",
    "    )\n",
    "\n",
    "flatten = ks.layers.Flatten()(vgg.output)\n",
    "dense1 = ks.layers.Dense(512, activation='relu')(flatten)\n",
    "dense1 = ks.layers.BatchNormalization()(dense1)\n",
    "dense2 = ks.layers.Dense(256, activation='relu')(dense1)\n",
    "dense2 = ks.layers.BatchNormalization()(dense2)\n",
    "output = ks.layers.Dense(256, activation='relu')(dense2)\n",
    "\n",
    "encoder = ks.Model(vgg.input, output, name='Encoder')\n",
    "\n",
    "input_a = ks.layers.Input(name='anchor_input', shape=input_shape)\n",
    "input_p = ks.layers.Input(name='positive_input', shape=input_shape)\n",
    "input_n = ks.layers.Input(name='negative_input', shape=input_shape)\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    encoder(input_a),\n",
    "    encoder(input_p),\n",
    "    encoder(input_n),\n",
    ")\n",
    "\n",
    "siamese_network = ks.Model(\n",
    "    inputs=[input_a, input_p, input_n],\n",
    "    outputs=distances\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=ks.optimizers.Adam(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.utils.plot_model(siamese_model, to_file='keras_triplet.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = dataset.get_batch_list()\n",
    "siamese_model.fit(dummy_data, epochs=10, validation_data=dataset.get_batch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "# triplet model from scratch\n",
    "def tripletloss(model_anchor, model_positive, model_negative, margin=0.5):\n",
    "    distance1 = tf.sqrt(tf.reduce_mean(tf.pow(model_anchor - model_positive, 2), 1, keepdims=True))\n",
    "    distance2 = tf.sqrt(tf.reduce_mean(tf.pow(model_anchor - model_negative, 2), 1, keepdims=True))\n",
    "    return tf.reduce_mean(tf.maximum(distance1 - distance2 + margin, 0))\n",
    "\n",
    "def tripletloss_wrapper(loss_list, margin = 0.5):\n",
    "    return (tripletloss(loss_list[0], loss_list[1], loss_list[2], margin))\n",
    "\n",
    "def get_siamese_net(input_shape):\n",
    "    input_a = ks.Input(shape=input_shape)\n",
    "    input_p = ks.Input(shape=input_shape)\n",
    "    input_n = ks.Input(shape=input_shape)\n",
    "\n",
    "    vgg = ks.applications.VGG19(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        pooling='max'\n",
    "    )\n",
    "\n",
    "    flatten = ks.layers.Flatten()\n",
    "    encoder = ks.layers.Dense(265, activation='relu')\n",
    "\n",
    "    encoded_a = vgg(input_a)\n",
    "    encoded_a = flatten(encoded_a)\n",
    "    encoded_a = encoder(encoded_a)\n",
    "\n",
    "    encoded_p = vgg(input_p)\n",
    "    encoded_p = flatten(encoded_p)\n",
    "    encoded_p = encoder(encoded_p)\n",
    "\n",
    "    encoded_n = vgg(input_n)\n",
    "    encoded_n = flatten(encoded_n)\n",
    "    encoded_n = encoder(encoded_n)\n",
    "\n",
    "    Distance_layer_ap = ks.layers.Lambda(lambda x: tf.sqrt(tf.reduce_mean(tf.pow(x[0] - x[1], 2), 1, keepdims=True)))\n",
    "    Distance_layer_an = ks.layers.Lambda(lambda x: tf.sqrt(tf.reduce_mean(tf.pow(x[0] - x[1], 2), 1, keepdims=True)))\n",
    "    distance_ap = Distance_layer_ap([encoded_a, encoded_p])\n",
    "    distance_an = Distance_layer_an([encoded_a, encoded_n])\n",
    "\n",
    "    siamese_net = ks.Model(inputs=(input_a, input_p, input_n), outputs=(distance_ap, distance_an))\n",
    "    return siamese_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_siamese_net((350,250,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.utils.plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = np.random.normal(size=(3, 350, 250,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_net(input_size):\n",
    "#input_size = [350, 250, 3]\n",
    "    anker_input = ks.Input(input_size)\n",
    "    positive_input = ks.Input(input_size)\n",
    "    negative_input = ks.Input(input_size)\n",
    "\n",
    "    vgg = ks.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_size,\n",
    "        pooling='max'\n",
    "    )\n",
    "\n",
    "    encoder_layer = ks.layers.Dense(256, activation='relu')(vgg)\n",
    "\n",
    "    encoded_anker = model(anker_input)\n",
    "    encoded_positive = model(positive_input)\n",
    "    encoded_negative = model(negative_input)\n",
    "\n",
    "    Distance_Layer = ks.layers.Lambda(lambda x: tripletloss_wrapper(x))\n",
    "    triplet_loss = Distance_Layer([encoded_anker, encoded_positive, encoded_negative])\n",
    "\n",
    "    similarity = ks.layers.Dense(1, activation='sigmoid')(triplet_loss)\n",
    "\n",
    "    siamese_net = ks.Model(inputs=[anker_input, positive_input, negative_input], outputs=similarity)\n",
    "\n",
    "    return siamese_net\n",
    "\n",
    "#encoder_layer = ks.layers.Dense(265, activation='relu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tripletloss(model_anchor, model_positive, model_negative, margin=0.5):\n",
    "    distance1 = tf.sqrt(tf.reduce_mean(tf.pow(model_anchor - model_positive, 2), 1, keepdims=True))\n",
    "    distance2 = tf.sqrt(tf.reduce_mean(tf.pow(model_anchor - model_negative, 2), 1, keepdims=True))\n",
    "    return tf.reduce_mean(tf.maximum(distance1 - distance2 + margin, 0))\n",
    "\n",
    "def tripletloss_wrapper(loss_list, margin = 0.5):\n",
    "    return (tripletloss(loss_list[0], loss_list[1], loss_list[2], margin))\n",
    "\n",
    "def make_triplet_loss_network(input_size):\n",
    "    \n",
    "    input_anker = ks.Input(input_size)\n",
    "    input_positive = ks.Input(input_size)\n",
    "    input_negative = ks.Input(input_size)\n",
    "\n",
    "    model = ks.Sequential()\n",
    "    model.add(\n",
    "        ks.applications.VGG16(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=input_size,\n",
    "            pooling='max'\n",
    "    )\n",
    "    )\n",
    "    model.add(ks.layers.Flatten())\n",
    "    model.add(ks.layers.Dense(256, activation='sigmoid'))\n",
    "\n",
    "    encoded_anker = model(input_anker)\n",
    "    encoded_positive = model(input_positive)\n",
    "    encoded_negative = model(input_negative)\n",
    "\n",
    "    triplet_model = ks.Model(inputs=[input_anker, input_positive, input_negative], outputs=[encoded_anker, encoded_positive, encoded_negative])\n",
    "\n",
    "    #loss = tripletloss(encoded_anker, encoded_positive, encoded_negative)\n",
    "\n",
    "    \n",
    "\n",
    "    # similarity_layer = ks.layers.Lambda(lambda x: tripletloss(x[0], x[1], x[2]))\n",
    "    # similarity_score = similarity_layer([encoded_anker, encoded_positive, encoded_negative])\n",
    "\n",
    "    return triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model = make_triplet_loss_network([350,250,3])\n",
    "\n",
    "triplet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet = dataset.get_batch_list()\n",
    "\n",
    "len(triplet[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model.fit(x=dataset.get_batch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_triplet_loss_network([350,250,3])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anker, positive, negative = dataset.get_triplet()\n",
    "\n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tfrecords dataset\n",
    "image_feature_description = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "    \"class\": tf.io.FixedLenFeature([], tf.int64), \n",
    "    }\n",
    "\n",
    "def _parse_data(unparsed_example):\n",
    "    return tf.io.parse_single_example(unparsed_example, image_feature_description)\n",
    "\n",
    "def _bytestring_to_pixels(parsed_example):\n",
    "    byte_string = parsed_example['image']\n",
    "    image = tf.io.decode_image(byte_string)\n",
    "    image = tf.reshape(image, input_shape)\n",
    "    return image, parsed_example[\"class\"]\n",
    "\n",
    "def load_and_extract_images(filepath):\n",
    "    dataset = tf.data.TFRecordDataset(filepath)\n",
    "    dataset = dataset.map(_parse_data, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.map(_bytestring_to_pixels, num_parallel_calls=AUTOTUNE) # .cache()\n",
    "    return dataset\n",
    "\n",
    "def tripletloss(model_anchor, model_positive, model_negative, margin=2):\n",
    "    distance1 = tf.sqrt(tf.reduce_mean(tf.pow(model_anchor - model_positive, 2), 1, keepdims=True))\n",
    "    distance2 = tf.sqrt(tf.reduce_mean(tf.pow(model_anchor - model_negative, 2), 1, keepdims=True))\n",
    "    return tf.reduce_mean(tf.maximum(distance1 - distance2 + margin, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_and_extract_images(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_input_fn(dataset):\n",
    "    double_set = tf.data.Dataset(dataset.batch(2))\n",
    "    return double_set\n",
    "\n",
    "triplet_set = triplet_input_fn(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "test_set = train_dataset.take(1)\n",
    "for image, label in test_set:\n",
    "    print(image.shape)\n",
    "    imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = ks.Sequential()\n",
    "convnet.add(ks.layers.Input(input_shape))\n",
    "convnet.add(ks.layers.Conv2D(32, [7,7]))\n",
    "convnet.add(ks.layers.MaxPool2D())\n",
    "convnet.add(ks.layers.Conv2D(64, [5,5]))\n",
    "convnet.add(ks.layers.MaxPool2D())\n",
    "convnet.add(ks.layers.Conv2D(128, [3,3]))\n",
    "convnet.add(ks.layers.MaxPool2D())\n",
    "convnet.add(ks.layers.Flatten())\n",
    "convnet.add(ks.layers.Dense(256, activation=None))\n",
    "convnet.add(ks.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet.compile(optimizer='Adam', loss=tripletloss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    convnet.fit(dataset, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a07be377b96d9253e4aa960188004583a7778b7453cac0e090bec1a451b774b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cardrecognition')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
